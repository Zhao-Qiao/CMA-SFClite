{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a100b4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhaoqiao/miniconda3/envs/entity_tracking/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nnsight\n",
    "from nnsight import LanguageModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3af1d9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(\"gpt2\", device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b29ebd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>layer</th>\n",
       "      <th>head</th>\n",
       "      <th>nie</th>\n",
       "      <th>abs_nie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>-2.486696</td>\n",
       "      <td>2.486696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.148109</td>\n",
       "      <td>1.148109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.590734</td>\n",
       "      <td>0.590734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.564491</td>\n",
       "      <td>0.564491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.285976</td>\n",
       "      <td>0.285976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank  layer  head       nie   abs_nie\n",
       "0     1     10     9 -2.486696  2.486696\n",
       "1     2      9     7 -1.148109  1.148109\n",
       "2     3     11     8 -0.590734  0.590734\n",
       "3     4      9     5  0.564491  0.564491\n",
       "4     5      9     2 -0.285976  0.285976"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"data/gpt2-small_nurse_man_20251110_215002.csv\"\n",
    "corpus_path = \"data/WikiText.txt\"\n",
    "df = pd.read_csv(path, comment=\"#\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5115ae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_topk_heads(cma_csv: str, top_k: int):\n",
    "    \"\"\"\n",
    "    Read CMA CSV and return top-K (layer, head, nie) tuples.\n",
    "\n",
    "    Priority:\n",
    "      - If 'rank' exists, sort ascending by rank (1 = best)\n",
    "      - Else sort by 'abs_nie' (desc) if present, otherwise by 'nie' (desc)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(cma_csv, comment=\"#\")\n",
    "\n",
    "    if \"rank\" in df.columns:\n",
    "        df = df.sort_values(\"rank\", ascending=True)\n",
    "    elif \"abs_nie\" in df.columns:\n",
    "        df = df.sort_values(\"abs_nie\", ascending=False)\n",
    "    elif \"nie\" in df.columns:\n",
    "        df[\"abs_nie\"] = df[\"nie\"].abs()\n",
    "        df = df.sort_values(\"abs_nie\", ascending=False)\n",
    "    else:\n",
    "        raise ValueError(\"CMA CSV must have 'rank' or 'nie' / 'abs_nie' column.\")\n",
    "\n",
    "    df = df.dropna(subset=[\"layer\", \"head\"])\n",
    "    if \"nie\" in df.columns:\n",
    "        df = df.dropna(subset=[\"nie\"])\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"CMA CSV '{cma_csv}' has no valid rows after dropping NaNs.\")\n",
    "\n",
    "    sel = df.head(top_k)[[\"layer\", \"head\", \"nie\"]].values.tolist()\n",
    "    return [(int(L), int(h), float(nie)) for (L, h, nie) in sel]\n",
    "\n",
    "\n",
    "top_k = 5\n",
    "top_heads = read_topk_heads(path, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0d1df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_base = \"The nurse said that\"\n",
    "prompt_cf = \"The man said that\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0532ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_head_mask(head_indices, hidden_dim, head_dim):\n",
    "    x = torch.zeros(hidden_dim)              # 必须是 tensor\n",
    "    l = torch.tensor(head_indices)           # head indices \n",
    "    idx = l[:, None] * head_dim + torch.arange(head_dim)\n",
    "    idx = idx.reshape(-1)\n",
    "    \n",
    "    x[idx] = 1\n",
    "    return x.to(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23fc261a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 2, -0.2859764099121094),\n",
       " (9, 5, 0.5644912719726562),\n",
       " (9, 7, -1.1481094360351562),\n",
       " (10, 9, -2.486696243286133),\n",
       " (11, 8, -0.5907344818115234)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_heads = read_topk_heads(path, top_k)\n",
    "top_heads = sorted(top_heads, key=lambda x: (x[0], x[1]))   \n",
    "top_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f00e3f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_head_of_exp(model, example, csv_path, top_k):\n",
    "    base_examples = [e[\"base\"]for e in example]\n",
    "    cf_examples = [e[\"counterfactual\"]for e in example]\n",
    "    \n",
    "    attn_dim = model.config.n_embd // model.config.n_head\n",
    "    results = []\n",
    "    top_heads = read_topk_heads(csv_path, top_k)\n",
    "    # sort the heads based on layer and head index\n",
    "    top_heads = sorted(top_heads, key=lambda x: (x[0], x[1]))   \n",
    "    # merge to [{layer: [heads]}]\n",
    "    heads_by_layer = {}\n",
    "    for head in top_heads:\n",
    "        L, h, nie = head\n",
    "        heads_by_layer.setdefault(L, []).append(h)\n",
    "        \n",
    "    with model.edit() as edited:\n",
    "        for idx, (L, heads) in enumerate(heads_by_layer.items()):\n",
    "            head_mask = get_head_mask(heads, model.config.n_embd, attn_dim)\n",
    "            edited.transformer.h[L].attn.c_proj.input[:, :, head_mask] = 0.0\n",
    "    with model.trace(base_examples) as t1:\n",
    "        base_logits = model.output['logits'][:,-1,:].detach().cpu().save()\n",
    "    with model.trace(cf_examples) as t2:\n",
    "        cf_logits = model.output['logits'][:,-1,:].detach().cpu().save()   \n",
    "    with edited.trace(base_examples) as et1:\n",
    "        edited_base_logits = model.output['logits'][:,-1,:].detach().cpu().save()\n",
    "    with edited.trace(cf_examples) as et2:\n",
    "        edited_cf_logits = model.output['logits'][:,-1,:].detach().cpu().save()\n",
    "        \n",
    "    return (edited, base_logits, cf_logits, edited_base_logits, edited_cf_logits)\n",
    "    \n",
    "    \n",
    "# result = run_head_of_exp(model, prompt_base, prompt_cf, path, top_k)\n",
    "# edited_model, base_logits, cf_logits, edited_base_logits, edited_cf_logits = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5694101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ppl(model, text):\n",
    "    \"\"\"Evaluate perplexity of the given text using the model.\"\"\"\n",
    "    inputs = model.tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    with model.trace(inputs):\n",
    "        logits = model.output['logits'].save()\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = inputs['input_ids'][..., 1:].contiguous()\n",
    "    loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                    shift_labels.view(-1))\n",
    "    ppl = torch.exp(loss.mean()).item()\n",
    "    return ppl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d3c5bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(\n",
    "    model,\n",
    "    path,\n",
    "    max_corpus_tokens\n",
    "):\n",
    "    if not path or not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    tokens = model.tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    if tokens.numel() <= 1:\n",
    "        return None\n",
    "    if max_corpus_tokens is not None and tokens.size(1) > max_corpus_tokens:\n",
    "        tokens = tokens[:, :max_corpus_tokens]\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfa6d570",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_corpus_perplexity(\n",
    "    model,\n",
    "    tokens,\n",
    "    block_size: int = 512,\n",
    ") -> float:\n",
    "    if tokens is None or tokens.numel() <= 1:\n",
    "        return float(\"nan\")\n",
    "    total_log_prob = 0.0\n",
    "    total_tokens = 0\n",
    "    device = model.device\n",
    "    for start in range(0, tokens.size(1) - 1, block_size):\n",
    "        end = min(start + block_size + 1, tokens.size(1))\n",
    "        slice_tokens = tokens[:, start:end]\n",
    "        with model.trace(slice_tokens) as t:\n",
    "            logits = model.output['logits'].save()\n",
    "        log_probs = torch.nn.functional.log_softmax(logits[:, :-1], dim=-1)\n",
    "        target = slice_tokens[:, 1:]\n",
    "        slice_tokens = slice_tokens.to(device)\n",
    "        target = slice_tokens[:, 1:].to(device)\n",
    "\n",
    "        gathered = torch.gather(log_probs, 2, target.unsqueeze(-1)).squeeze(-1) \n",
    "        total_log_prob += gathered.sum().item()\n",
    "        total_tokens += target.numel()\n",
    "    if total_tokens == 0:\n",
    "        return float(\"nan\")\n",
    "    avg_log_prob = total_log_prob / total_tokens\n",
    "    return float(np.exp(-avg_log_prob))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "eaf5cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokens = tokenize_corpus(model, corpus_path, max_corpus_tokens=4096)\n",
    "baseline_corpus_ppl = (compute_corpus_perplexity(model, corpus_tokens) if corpus_tokens is not None else float(\"nan\"))\n",
    "edited_corpus_ppl = (compute_corpus_perplexity(edited_model, corpus_tokens) if corpus_tokens is not None else float(\"nan\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b02b0cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20.934903423804844, 22.173661220600465)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_corpus_ppl, edited_corpus_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5964e2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:08<00:00,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "logs = []\n",
    "for k in tqdm([1, 5, 10, 20, 50, 100]):\n",
    "    log = {\n",
    "        \"k\": k,\n",
    "        \"NIE_before\": 0,\n",
    "        \"NIE_after\": 0,\n",
    "        \"ppl_before\": 0,\n",
    "        \"ppl_after\": 0,\n",
    "    }\n",
    "    result = run_head_of_exp(model, prompt_base, prompt_cf, path, k+1)\n",
    "    edited_model, base_logits, cf_logits, edited_base_logits, edited_cf_logits = result\n",
    "    id_she = model.tokenizer(\" she\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "    id_he = model.tokenizer(\" he\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "    bias_clean = (base_logits[0, id_she] - base_logits[0,id_he]).item()\n",
    "    bias_gated = (edited_base_logits[0,id_she] - edited_base_logits[0,id_he]).item()\n",
    "    bias_cf_replaced = (edited_cf_logits[0, id_she] - edited_cf_logits[0, id_he]).item()\n",
    "    nie_before = bias_cf_replaced - bias_clean\n",
    "    nie_after = bias_cf_replaced - bias_gated\n",
    "    log[\"NIE_before\"] = nie_before\n",
    "    log[\"NIE_after\"] = nie_after\n",
    "    corpus_tokens = tokenize_corpus(model, corpus_path, max_corpus_tokens=4096)\n",
    "    baseline_corpus_ppl = (compute_corpus_perplexity(model, corpus_tokens) if corpus_tokens is not None else float(\"nan\"))\n",
    "    edited_corpus_ppl = (compute_corpus_perplexity(edited_model, corpus_tokens) if corpus_tokens is not None else float(\"nan\"))\n",
    "    log[\"ppl_before\"] = baseline_corpus_ppl\n",
    "    log[\"ppl_after\"] = edited_corpus_ppl\n",
    "    remaining_pct = (\n",
    "            float(abs(bias_gated) / (abs(bias_clean) + 1e-9))\n",
    "            if abs(bias_clean) > 1e-9\n",
    "            else float(\"nan\")\n",
    "        )\n",
    "    log[\"remaining_pct\"] = remaining_pct\n",
    "    logs.append(log)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41a29937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts_winogender import get_prompt_examples\n",
    "examples = get_prompt_examples(\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0229e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = run_head_of_exp(model, examples, path, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7766bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "logs = []\n",
    "for k in tqdm(range(10)):\n",
    "    log = {\n",
    "        \"k\": k,\n",
    "        \"NIE_before\": 0,\n",
    "        \"NIE_after\": 0,\n",
    "        \"ppl_before\": 0,\n",
    "        \"ppl_after\": 0,\n",
    "    }\n",
    "    result = run_head_of_exp(model, examples, path, k)\n",
    "    edited_model, base_logits, cf_logits, edited_base_logits, edited_cf_logits = result\n",
    "    id_she = model.tokenizer(\" she\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "    id_he = model.tokenizer(\" he\", add_special_tokens=False)[\"input_ids\"][0]\n",
    "    bias_clean = (base_logits[:, id_she] - base_logits[:, id_he])\n",
    "    bias_edited = (edited_base_logits[:, id_she] - edited_base_logits[:, id_he])\n",
    "    bias_cf_replaced = (edited_cf_logits[:, id_she] - edited_cf_logits[:, id_he])\n",
    "    log['bias_clean_raw'] = bias_clean\n",
    "    log['bias_edited_raw'] = bias_edited\n",
    "    log['bias_cf_replaced_raw'] = bias_cf_replaced\n",
    "    \n",
    "    bias_clean = bias_clean.mean().item()\n",
    "    bias_edited = bias_edited.mean().item()\n",
    "    bias_cf_replaced = bias_cf_replaced.mean().item()\n",
    "    corpus_tokens = tokenize_corpus(model, corpus_path, max_corpus_tokens=4096)\n",
    "    baseline_corpus_ppl = (compute_corpus_perplexity(model, corpus_tokens) if corpus_tokens is not None else float(\"nan\"))\n",
    "    edited_corpus_ppl = (compute_corpus_perplexity(edited_model, corpus_tokens) if corpus_tokens is not None else float(\"nan\"))\n",
    "    log[\"ppl_before\"] = baseline_corpus_ppl\n",
    "    log[\"ppl_after\"] = edited_corpus_ppl\n",
    "    log['d_ppl'] = abs(edited_corpus_ppl - baseline_corpus_ppl)\n",
    "    remaining_pct = (\n",
    "            float(abs(bias_edited) / (abs(bias_clean) + 1e-9))\n",
    "            if abs(bias_clean) > 1e-9\n",
    "            else float(\"nan\")\n",
    "        )\n",
    "    log[\"remaining_pct\"] = remaining_pct\n",
    "    logs.append(log)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1edb7ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'k': 0,\n",
       "  'NIE_before': 0,\n",
       "  'NIE_after': 0,\n",
       "  'ppl_before': 20.934903423804844,\n",
       "  'ppl_after': 20.934903423804844,\n",
       "  'bias_clean_raw': tensor([3.1397, 0.8581, 4.7102, 1.6012, 0.3988]),\n",
       "  'bias_edited_raw': tensor([3.1397, 0.8581, 4.7102, 1.6012, 0.3988]),\n",
       "  'bias_cf_replaced_raw': tensor([-2.4131, -1.2378, -4.3987, -0.4387, -2.0944]),\n",
       "  'd_ppl': 0.0,\n",
       "  'remaining_pct': 0.9999999995330601},\n",
       " {'k': 1,\n",
       "  'NIE_before': 0,\n",
       "  'NIE_after': 0,\n",
       "  'ppl_before': 20.934903423804844,\n",
       "  'ppl_after': 20.95264363222214,\n",
       "  'bias_clean_raw': tensor([3.1397, 0.8581, 4.7102, 1.6012, 0.3988]),\n",
       "  'bias_edited_raw': tensor([ 0.7468, -0.1749,  2.8602,  1.2758,  0.3267]),\n",
       "  'bias_cf_replaced_raw': tensor([-1.5976, -1.1484, -2.8746, -0.1965, -2.0159]),\n",
       "  'd_ppl': 0.01774020841729751,\n",
       "  'remaining_pct': 0.4701681683243988},\n",
       " {'k': 2,\n",
       "  'NIE_before': 0,\n",
       "  'NIE_after': 0,\n",
       "  'ppl_before': 20.934903423804844,\n",
       "  'ppl_after': 21.1700966252794,\n",
       "  'bias_clean_raw': tensor([3.1397, 0.8581, 4.7102, 1.6012, 0.3988]),\n",
       "  'bias_edited_raw': tensor([-0.9914, -0.5887,  1.2745,  1.2484, -0.3072]),\n",
       "  'bias_cf_replaced_raw': tensor([-2.0187, -1.0331, -2.4152,  0.0374, -1.8083]),\n",
       "  'd_ppl': 0.23519320147455502,\n",
       "  'remaining_pct': 0.059360873950410266},\n",
       " {'k': 3,\n",
       "  'NIE_before': 0,\n",
       "  'NIE_after': 0,\n",
       "  'ppl_before': 20.934903423804844,\n",
       "  'ppl_after': 21.801860919505028,\n",
       "  'bias_clean_raw': tensor([3.1397, 0.8581, 4.7102, 1.6012, 0.3988]),\n",
       "  'bias_edited_raw': tensor([-0.8788, -0.3596,  1.2246,  0.6380, -0.3079]),\n",
       "  'bias_cf_replaced_raw': tensor([-1.9945, -0.7142, -2.4114, -0.4026, -2.0127]),\n",
       "  'd_ppl': 0.8669574957001842,\n",
       "  'remaining_pct': 0.029541991245689625},\n",
       " {'k': 4,\n",
       "  'NIE_before': 0,\n",
       "  'NIE_after': 0,\n",
       "  'ppl_before': 20.934903423804844,\n",
       "  'ppl_after': 22.24532741194345,\n",
       "  'bias_clean_raw': tensor([3.1397, 0.8581, 4.7102, 1.6012, 0.3988]),\n",
       "  'bias_edited_raw': tensor([-0.8716, -0.4354,  1.0134, -0.1902, -0.3155]),\n",
       "  'bias_cf_replaced_raw': tensor([-1.9937, -0.8111, -2.8511, -1.0620, -2.0474]),\n",
       "  'd_ppl': 1.3104239881386057,\n",
       "  'remaining_pct': 0.07464922764427534},\n",
       " {'k': 5,\n",
       "  'NIE_before': 0,\n",
       "  'NIE_after': 0,\n",
       "  'ppl_before': 20.934903423804844,\n",
       "  'ppl_after': 22.173661220600465,\n",
       "  'bias_clean_raw': tensor([3.1397, 0.8581, 4.7102, 1.6012, 0.3988]),\n",
       "  'bias_edited_raw': tensor([-1.2122, -0.6838,  0.5302,  0.0513, -0.4846]),\n",
       "  'bias_cf_replaced_raw': tensor([-2.1385, -1.1117, -2.9789, -0.8799, -2.0322]),\n",
       "  'd_ppl': 1.2387577967956211,\n",
       "  'remaining_pct': 0.16802061792964373},\n",
       " {'k': 6,\n",
       "  'NIE_before': 0,\n",
       "  'NIE_after': 0,\n",
       "  'ppl_before': 20.934903423804844,\n",
       "  'ppl_after': 22.31541112048494,\n",
       "  'bias_clean_raw': tensor([3.1397, 0.8581, 4.7102, 1.6012, 0.3988]),\n",
       "  'bias_edited_raw': tensor([-1.6240, -0.6894,  0.3953, -0.2838, -0.2876]),\n",
       "  'bias_cf_replaced_raw': tensor([-2.1059, -1.1593, -2.9387, -0.9301, -2.0642]),\n",
       "  'd_ppl': 1.3805076966800947,\n",
       "  'remaining_pct': 0.23249398878277241},\n",
       " {'k': 7,\n",
       "  'NIE_before': 0,\n",
       "  'NIE_after': 0,\n",
       "  'ppl_before': 20.934903423804844,\n",
       "  'ppl_after': 22.452068234144622,\n",
       "  'bias_clean_raw': tensor([3.1397, 0.8581, 4.7102, 1.6012, 0.3988]),\n",
       "  'bias_edited_raw': tensor([-0.7698, -0.1934,  0.9476,  0.0565, -0.7803]),\n",
       "  'bias_cf_replaced_raw': tensor([-1.5578, -0.6770, -1.7968, -0.5166, -1.7726]),\n",
       "  'd_ppl': 1.5171648103397786,\n",
       "  'remaining_pct': 0.06905074604881964},\n",
       " {'k': 8,\n",
       "  'NIE_before': 0,\n",
       "  'NIE_after': 0,\n",
       "  'ppl_before': 20.934903423804844,\n",
       "  'ppl_after': 22.766297974370545,\n",
       "  'bias_clean_raw': tensor([3.1397, 0.8581, 4.7102, 1.6012, 0.3988]),\n",
       "  'bias_edited_raw': tensor([-1.0253, -0.1741,  0.7901, -0.3466, -0.8937]),\n",
       "  'bias_cf_replaced_raw': tensor([-2.0635, -0.6506, -1.7765, -0.7744, -1.8591]),\n",
       "  'd_ppl': 1.831394550565701,\n",
       "  'remaining_pct': 0.15404540258495894},\n",
       " {'k': 9,\n",
       "  'NIE_before': 0,\n",
       "  'NIE_after': 0,\n",
       "  'ppl_before': 20.934903423804844,\n",
       "  'ppl_after': 22.834808520935894,\n",
       "  'bias_clean_raw': tensor([3.1397, 0.8581, 4.7102, 1.6012, 0.3988]),\n",
       "  'bias_edited_raw': tensor([-1.5052, -0.5551,  0.1248, -0.7623, -1.3827]),\n",
       "  'bias_cf_replaced_raw': tensor([-2.2815, -0.9383, -1.9197, -1.0756, -1.7772]),\n",
       "  'd_ppl': 1.8999050971310503,\n",
       "  'remaining_pct': 0.3810701953659496}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9579696",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = run_head_of_exp(model,examples, path, 3)\n",
    "edited_model, base_logits, cf_logits, edited_base_logits, edited_cf_logits = result1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "842a8732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base PPL: 61.7552490234375, Edited PPL: 69.33033752441406, Diff: 7.5750885009765625\n",
      "Base PPL: 122.14643859863281, Edited PPL: 136.76490783691406, Diff: 14.61846923828125\n",
      "Base PPL: 88.93116760253906, Edited PPL: 86.7463150024414, Diff: -2.1848526000976562\n"
     ]
    }
   ],
   "source": [
    "# ppl on examples\n",
    "val_example = get_prompt_examples(\"val\")\n",
    "for v in val_example:\n",
    "    edited_ppl = eval_ppl(edited_model, v['base'])\n",
    "    base_ppl = eval_ppl(model, v['base'])\n",
    "    print(f\"Base PPL: {base_ppl}, Edited PPL: {edited_ppl}, Diff: {edited_ppl - base_ppl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "648489ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def compute_ppl_sentence_by_sentence(model, sentences):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_nll = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "\n",
    "    for s in tqdm(sentences):\n",
    "        with model.trace(s):\n",
    "            output = model.output.save()\n",
    "        logits = output['logits']\n",
    "        inputs = model.tokenizer(s, return_tensors=\"pt\").to(model.device)\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = inputs['input_ids'][..., 1:].contiguous()\n",
    "        loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                        shift_labels.view(-1))\n",
    "        nll = loss.sum().item()\n",
    "        num_tokens = shift_labels.numel()\n",
    "        total_nll += nll\n",
    "        total_tokens += num_tokens\n",
    "    ppl = torch.exp(torch.tensor(total_nll / total_tokens)).item()\n",
    "    return ppl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a05b9d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = get_prompt_examples(\"train\")\n",
    "train_b = [e['base'] for e in train_examples]\n",
    "train_cf = [e['counterfactual'] for e in train_examples]\n",
    "val_examples = get_prompt_examples(\"val\")\n",
    "val_b = [e['base'] for e in val_examples]\n",
    "val_cf = [e['counterfactual'] for e in val_examples]\n",
    "sentences = train_b + train_cf + val_b + val_cf\n",
    "model_name = \"gpt2\"\n",
    "output = compute_ppl_sentence_by_sentence(model, sentences)\n",
    "output_ed = compute_ppl_sentence_by_sentence(edited_model, sentences)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entity_tracking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
